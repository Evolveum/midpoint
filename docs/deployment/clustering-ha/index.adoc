---
midpoint-feature: ha
doc-type: config
---
= Achieve High Availability of MidPoint with Clustering
:page-nav-title: High Availability with Clustering
:page-wiki-name: Clustering / high availability setup
:page-wiki-id: 11075783
:page-wiki-metadata-create-user: mederly
:page-wiki-metadata-create-date: 2013-06-28T10:24:03.273+02:00
:page-wiki-metadata-modify-user: mederly
:page-wiki-metadata-modify-date: 2020-09-23T16:15:32.817+02:00
:page-upkeep-status: green
:page-toc: top
:page-moved-from: /midpoint/reference/deployment/ha/
:page-description: Use a cluster of multiple nodes to create scalable, robust, and highly available midPoint deployment
:page-keywords: clustering, high availability, node configuration, load balancing
// TODO redirects in the Reference don't work currently. @dakle 2025-07-14

To achieve a high availability deployment, set up a cluster of several midPoint nodes working with a common midPoint repository.
You can combine the multi-node deployment with a load balancer to evenly distribute the GUI and REST API traffic load across the nodes.


== Architecture of MidPoint Cluster

There can be two or more nodes in a cluster.
Although there is no fixed limit to the number of nodes in a cluster, practical limits always exist;
it is up to you to determine the viable number of nodes experimentally based on your needs.

In the following schema, there are two midPoint nodes with a shared central repository and a load balancer.

.A midPoint deployment with two nodes, a shared repository, and an optional load balancer
image::midpoint-HA-deployment-schema-two-nodes.svg["Two-node midPoint deployment schema with two midPoint nodes and their shared repository in the center; lines showing access routes between individual components and user's computer. Optional load balancer is placed between midPoint and the computer"]

The nodes in the cluster share the load as follows:

* MidPoint tasks, such as live synchronization, reconciliation, import, or workflow approvals, can run on whichever node.
    The nodes take the available worker tasks on the first-come-first-served basis which results in a reasonably even load distribution across the cluster.
* Users connecting to the graphical user interface (GUI) can work on any node.

Here is an outline of technologies used and recommended for the work distribution:

* For task scheduling and distribution, midPoint uses link:http://www.quartz-scheduler.org[Quartz scheduler].
    See xref:/midpoint/reference/tasks/task-manager/[] and <<intra-cluster-communication,Communication in Clusters Explained>> for architectural description and implementation details of the solution.
* For load balancing the traffic to the GUI and REST API, we recommend using the standard link:http://tomcat.apache.org/tomcat-8.0-doc/balancer-howto.html[Apache Tomcat solution].
* If you wish to fail over GUI sessions without load balancing, you can use a network-level setup with a virtual IP.

== Configure Your MidPoint Cluster

In order to deploy midPoint in a cluster, you need to adjust a couple of parameters in the midPoint configuration.
This section covers the all the required settings you need to get you started.

=== Basic Setup

Here is a bare-bones configuration to start with when using the default PostgreSQL database:

[source,xml]
----
<repository>
  <repositoryServiceFactoryClass>com.evolveum.midpoint.repo.sql.SqlRepositoryFactory</repositoryServiceFactoryClass>
  <database>postgresql</database>
  <jdbcUsername>midpoint</jdbcUsername>
  <jdbcPassword>.....</jdbcPassword>
  <jdbcUrl>jdbc:postgresql://..../midpoint</jdbcUrl>
  <hibernateHbm2ddl>none</hibernateHbm2ddl>
  <missingSchemaAction>create</missingSchemaAction>
</repository>
<taskManager>
  <clustered>true</clustered> <1>
</taskManager>
----
<1> The `<clustered>` attribute signifies the installation uses clustered mode.
Default is `false`.
If you need clustering for high availability and failover, set it to `true`.

.Clustering needs to be enabled on all nodes
[WARNING]
====
All nodes sharing the same repository must have the attribute `clustered` set to `true`.
If some are set up differently, tasks will not be scheduled and distributed correctly.
MidPoint disables scheduling tasks on non-conformant nodes, i.e., on non-clustered nodes that are parts of the system.
The best way how to avoid configuration discrepancy like this is to have common configuration file.
If that is not possible or practical, make sure that all nodes use the same settings.
====

In some circumstances, the Quartz component in task manager needs to use a separate database.
If that's the case, a proper configuration is needed.

When deploying clustered nodes, ensure your *system time is synchronized* across all nodes (using link:https://en.wikipedia.org/wiki/Network_Time_Protocol[NTP] or similar service).
Otherwise, unexpected behaviour may occur, such as tasks restarting on different nodes.

=== Node Identification and  Discovery Configuration

Even though the nodes in your cluster mainly talk to the shared database repository, at times, they <<intra-cluster-communication,need to communicate with each other>> as well.
For this reason, you need to adjust their identification and discovery configuration.

You can use the xref:/midpoint/reference/repository/configuration/[repository configuration in the `config.xml` file] for your node configuration (see the _Repository configuration_ column in the table below).
In this case, place the configuration keys directly into the `<midpoint>` element.

Alternatively, you can specify the configuration via the link:https://en.wikipedia.org/wiki/Command-line_interface[command line interface] (CLI) parameters in the form of `-Dkey=value`.
See the _CLI_ column in the table below.
Refer to xref:/midpoint/reference/deployment/midpoint-home-directory/overriding-config-xml-parameters/[] for details.

.Parameters specifying node identification and discovery options in a clustered midPoint deployment
[%autowidth]
|===
| CLI | Repository configuration | Description

| `‑Dmidpoint.nodeId`
| `nodeId`
| The node identifier.
The default is `DefaultNode` for non-clustered deployments.
For clustered ones, either `nodeId` or `nodeIdSource` must be specified.

| `‑Dmidpoint.nodeIdSource`
| `nodeIdSource`
| Source of the node identifier.
It is applied if `nodeId` is not defined explicitly.
The source can be either:

* `hostname`: The host name is used as the node identifier.
* `random`: A random value for node ID is generated when the node starts.

This attribute is obsolete.
You can still use it, though, if you manage nodes manually, need deterministic node IDs for testing, or you are migrating from an old midPoint version and need to preserve existing node IDs.

| `‑Dmidpoint.hostName`
| `hostName`
| Overrides the local host name information.
If not specified, the operating system is used to determine the host name.

Normally, you do not need to specify this information.


| `‑Dmidpoint.httpPort`
| `httpPort`
| Overrides the local HTTP port information.
If not specified, Tomcat/Catalina is queried to determine the HTTP port information.
This information is used only to construct URL address for <<intra-cluster-communication,intra-cluster communication>>.


If you run a node behind a reverse proxy or link:https://en.wikipedia.org/wiki/Network_address_translation[NAT], for instance, you need to specify the port based on the network configuration.
In such a case, you always need to specify the port number under which other nodes can see the particular node from their point of view.
Normally, you do not need to specify this information.

If you want to run midPoint under a custom port, use xref:/midpoint/operations-manual/#changing-the-midpoint-embedded-tomcat-configuration[`‑Dserver.port=xxx`] instead.


| `‑Dmidpoint.url`
| `url`
| Overrides the <<how-intra-cluster-urls-are-determined,intra-cluster URL pattern>>.
Normally, you do not need to specify this information.

|===

=== How Intra-Cluster URLs Are Determined

In order to minimize the configuration work needed while keeping the maximum level of flexibility,
the node URLs used for intra-cluster communication (e.g., `https://node1.acme.org:8080/midpoint`) are derived from the following items in the order listed here:

. `*<urlOverride>*` property in the node object in the repository.
. `*-Dmidpoint.url*` / `*<url>*` information (CLI parameter or `config.xml` file).
. Computed based on the information in the `*infrastructure/intraClusterHttpUrlPattern*` property, if defined. +
    This property can use the following macros:
    ** `*$host*` for host name: obtained dynamically from the OS or overridden by the `-Dmidpoint.hostname` or `<hostname>` config properties.
    ** `*$port*` for HTTP port: obtained dynamically from Tomcat objects or overridden by `-Dmidpoint.httpPort` or `<httpPort>` config properties.
    ** `*$path*` for midPoint URL path: obtained dynamically from the servlet container.
. Computed based on the protocol scheme obtained dynamically from the Tomcat objects, host name, port, and servlet path as `scheme://host:port/path`.

When troubleshooting these mechanisms, you can set logging to `DEBUG` for `com.evolveum.midpoint.task.quartzimpl.cluster.NodeRegistrar` (or the whole task manager module).

=== Define URL pattern for inter-node communication

Nodes use the HTTP URL pattern to communicate between themselves.
The pattern is a URL prefix pointing to the root URL of midPoint.
Below is an example definition for the system configuration object:
// NOTE: I'm intentionally using HTTP instead of HTTPS because the protocol is, per se, HTTP.
//      Regardless of whether or not it is secured by a TLS or SSL, because those are separate technologies. @dakle 2025-07-09

[source,xml]
----
<systemConfiguration>
  ...
  <infrastructure>
    <intraClusterHttpUrlPattern>https://$host/midpoint</intraClusterHttpUrlPattern>
  </infrastructure>
  ...
</systemConfiguration>
----

== Test Cluster Configuration on a Single Host

To test a cluster configuration on a single host (with nodes running on different ports),
use the configuration below.
This configuration allows more nodes to use a single IP address,
so that cluster containing nodes on a single host can be formed.
This feature is experimental.

[source,xml]
----
<taskManager>
  <localNodeClusteringEnabled>true</localNodeClusteringEnabled>
</taskManager>
----

In CLI, use `-Dmidpoint.taskManager.localNodeClusteringEnabled=true`.

[[cache-invalidation]]
[[intra-cluster-communication]]
== Communication in Cluster Explained

// TODO (parts of) this may need to go rather to the Task Manager - /midpoint/reference/tasks/task-manager/
// WiP
Cluster nodes primarily communicate with the central shared database.
Tasks for the nodes to process are stored in this database.
The data on which nodes operate when processing the tasks are stored in the database as well.
Each task is split to xref:/midpoint/reference/tasks/activities/distribution/#buckets[buckets] based on a key in the task definition.
When the time to start a task comes, worker tasks (also called child tasks) are created.
When picked by a node, the worker task selects an available bucket and processes it on the node.

Each node runs its own Quartz Scheduler library.
The Quartz library is responsible for the node to pick up available worker tasks and buckets, as well as to prevent any processing collisions with other nodes by storing the runtime information in the xref:/midpoint/reference/tasks/task-manager/configuration/#jdbc-scheduler-job-store[JDBC scheduler job store] in the repository.
To summarize, *all communication regarding work distribution happens between the central database and the nodes*.

However, there are *situations when nodes need to talk to each other* directly.
A notable occasion requiring node-to-node communication is *cache invalidation*.
When a node changes data in the midPoint database, the node informs other nodes about the need to invalidate their cache.
See also <<technical-insight-into-cache,Technical Insight into Cache>>.

Another reason for nodes to communicate directly is user session handling.
After an operation on one node changes user attributes, such as assigned roles or permissions,
the node propagates this information to other nodes to let them know
they need to update their information on what the user can or cannot do.
They may need to drop the session altogether if the user has been deactivated.

These situations requiring direct node-to-node communication are the reason why you need to specify an HTTP URL pattern.
It is used by midPoint nodes to communicate among themselves.

[NOTE]
====
Since midPoint 4.0, nodes communicate over HTTP instead of JMX.
====

=== You May Get Redirected Between Nodes

To help you understand the intra-cluster communication further,
here is an example of a situation when direct node-to-node communication does not happen, although you may expect it would.

If a node runs a task to feature:reporting[create a report], for example, the resulting report file is saved on the local file system of the node.
If user sitting on a different node requests the report for download,
the node on which the user is asks the central DB for the location of the report
and then redirects the user to the node with the generated report.
Hence, inter-node communication does not occur in this case.

== Technical Insight into Cache

// TODO: This article is not likely the most suitable place for this section but I don't know where else to put it. @dakle 2025-07-20

MidPoint uses two levels of cache: global and local.

The local cache is per task thread.
It holds query objects with results, all touched objects, and version cache, which consists of all versions of modified objects.
(Every time an object is modified, a new version of it is created.)

The global cache is per node and holds objects that don't change often but are accessed very often.
These are, for example, system configuration, archetypes, or object templates.
These objects are cheap to cache because they don't change often, but saving them in cache saves a lot of resources.
User objects are not cached because they change often, but are rarely needed.

== Common Issues and Fixes

These are the critical criteria your configuration must meet:

* Use a shared repository.
    All nodes must connect to the same repository.
* Define node URLs using the `<midpoint><url>...</url></midpoint>` or `intraClusterHttpUrlPattern` configuration options in the system configuration to ensure nodes can discover each other.
* Clustering in production requires an link:https://evolveum.com/services/support-subscriptions/[active subscription] (log error: _Clustering is not supported in production mode without a subscription_).

Here are a few common issues, their possible causes, and tips on how to resolve them:

. *Unauthorized errors (401)*
    ** Cause: Missing or invalid subscription ID or misconfigured REST authentication.
    ** Fix: Set a valid subscription ID in System > System Configuration > Deployment Information > Subscriptions Identifier. Ensure nodes can authenticate via REST (e.g., shared secrets or OAuth2 if configured).
. *Node discovery failures*
    ** Cause: Incorrect `intraClusterHttpUrlPattern` setting or firewall rules blocking HTTP(S) traffic.
    ** Fix: Check your deployment configuration and all possibly related network settings. Test connectivity between nodes using `curl` or a similar tool.
. *Sticky sessions*
    ** Cause: Load balancer is not using sticky sessions (e.g., `ip_hash` in NGINX).
    ** Fix: Configure the load balancer to maintain session affinity (e.g., by using link:https://en.wikipedia.org/wiki/Load_balancing_(computing)#Persistence[sticky cookie] or source IP).
. *Database locks or task scheduling issues*
    ** Cause: Inconsistent `clustered=true` setting across nodes.
    ** Fix: Ensure all nodes have the consistently set `clustered=true`.


== Limitations

Clustering functionality assumes *homogeneous* cluster environment.
That means each cluster node must have the same environment, configuration, connectivity (e.g., to load balancer), connectors and so on.
Clustering implementation assumes that a task can be executed on any cluster node, giving the same result regardless.
Any configuration differences between cluster nodes are likely to cause operational issues.

The following aspects must be the same on all cluster nodes:

* Versions of
    ** MidPoint
    ** Connectors
    ** Schema extension​footnote:[
        Only when stored in an XSD configuration file.
        Since midPoint 4.9, schema extensions can be stored in the shared database. Refer to xref:/midpoint/reference/schema/custom-schema-extension/[] for details.]
    ** Java key store and trust store
* Network access to all configured resources
* Access to file systems, including network file systems (e.g., for CSV resources)
* Network configuration, including routing and DNS configuration
// I decided not to capitalize Java key store and trust store
// because the official docs on them uses all the possible spacing and capitalization options.
// Hence, I went with my grammar gut.
// https://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html

== See Also

* xref:/midpoint/reference/tasks/task-manager/[]

* xref:/midpoint/reference/tasks/activities/distribution/[]

* xref:/midpoint/install/system-requirements/[]
